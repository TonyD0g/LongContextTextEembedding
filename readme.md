# 长上下文文本嵌入解决方案

## 前言

在搭建基于RAG的AI知识库时，文本嵌入的质量直接决定了知识检索的准确性。通常，嵌入模型（如`text-embedding-v4`）有8192 tokens的输入限制。

当处理报告、长篇文章等超出此限制的文本时，直接嵌入会触发错误。

本库深入探讨了此问题的成因，并提供了多种解决方案，例如借鉴了OpenAI的“长度安全嵌入”思路，通过智能分块与向量合并来优雅地解决这一难题



## 使用

- `extClassStruct.py` 中填写QWEN_GPT_API_KEY的值

- 执行`main.py` 文件